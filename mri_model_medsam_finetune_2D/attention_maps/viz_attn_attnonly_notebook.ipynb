{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4528efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Imports & helpers\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib  # for NIfTI lesion masks\n",
    "\n",
    "# Models / dataset\n",
    "from segment_anything import sam_model_registry\n",
    "from ISUPMedSAM import IMG_SIZE, MedSAMSliceSpatialAttn, resize_to_img_size\n",
    "from dataset_picai_slices import PicaiSliceDataset\n",
    "\n",
    "# --- plotting defaults\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# ===================== Utils =====================\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def to_uint8_gray(channel_tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a single-channel tensor in [0,1] with shape (1,H,W) or (H,W)\n",
    "    into an HxWx3 uint8 grayscale image.\n",
    "    \"\"\"\n",
    "    if channel_tensor.ndim == 3 and channel_tensor.shape[0] == 1:\n",
    "        ch = channel_tensor[0]\n",
    "    else:\n",
    "        ch = channel_tensor\n",
    "    x = ch.detach().cpu().clamp(0, 1).numpy()\n",
    "    x = (x * 255.0).round().astype(np.uint8)\n",
    "    return np.stack([x, x, x], axis=-1)\n",
    "\n",
    "def normalize01(arr: torch.Tensor | np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize a tensor/ndarray to [0,1] (returns numpy).\n",
    "    If nearly constant, returns zeros.\n",
    "    \"\"\"\n",
    "    a = arr.detach().cpu().numpy() if torch.is_tensor(arr) else arr\n",
    "    mn, mx = float(a.min()), float(a.max())\n",
    "    if mx <= mn + eps:\n",
    "        return np.zeros_like(a, dtype=np.float32)\n",
    "    return ((a - mn) / (mx - mn)).astype(np.float32)\n",
    "\n",
    "def overlay_red(img_uint8_hwc: np.ndarray, heat01_hw: np.ndarray, alpha: float = 0.25) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Faint red overlay (alpha default 0.25) on an RGB base image (uint8).\n",
    "    heat01_hw should be in [0,1] with shape HxW.\n",
    "    \"\"\"\n",
    "    base = img_uint8_hwc.astype(np.float32) / 255.0\n",
    "    heat = np.clip(heat01_hw, 0.0, 1.0)[..., None]\n",
    "    red = np.concatenate([heat, np.zeros_like(heat), np.zeros_like(heat)], axis=-1)\n",
    "    out = (1 - alpha) * base + alpha * red\n",
    "    return (out * 255.0).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "def rotate_ccw(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rotate HxW or HxWx3 array 90° counterclockwise for display.\n",
    "    \"\"\"\n",
    "    return np.rot90(arr, k=1, axes=(0, 1))\n",
    "\n",
    "def dbg_stats(name: str, hm: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Print quick diagnostics for heatmaps:\n",
    "    per-image min/max/std and mass in top-5% pixels (uniform ≈ 0.05).\n",
    "    Accepts [B,H,W] or [B,1,H,W].\n",
    "    \"\"\"\n",
    "    if hm.ndim == 4:\n",
    "        hm = hm.squeeze(1)\n",
    "    flat = hm.reshape(hm.shape[0], -1)\n",
    "    mins = flat.min(dim=1).values[:8]\n",
    "    maxs = flat.max(dim=1).values[:8]\n",
    "    stds = flat.std(dim=1)[:8]\n",
    "    k = max(1, int(flat.shape[1] * 0.05))\n",
    "    topk_mass = torch.topk(flat, k, dim=1).values.sum(dim=1) / flat.sum(dim=1).clamp(min=1e-12)\n",
    "    print(f\"[{name}] min[:8]={mins.tolist()}\")\n",
    "    print(f\"[{name}] max[:8]={maxs.tolist()}\")\n",
    "    print(f\"[{name}] std[:8]={stds.tolist()}\")\n",
    "    print(f\"[{name}] top5% mass (uniform≈0.05)[:8]={topk_mass.tolist()}\")\n",
    "\n",
    "# ===================== Model helpers =====================\n",
    "\n",
    "def build_model(\n",
    "    sam_type: str,\n",
    "    sam_ckpt: str,\n",
    "    num_classes: int = 3,\n",
    "    proj_dim: int = 512,\n",
    "    attn_dim: int = 256,\n",
    "    head_hidden: int = 256,\n",
    "    head_dropout: float = 0.0,   # keep 0.0 for eval\n",
    "    use_pre_neck: bool = True,\n",
    "    device: str = \"cuda\",\n",
    ") -> MedSAMSliceSpatialAttn:\n",
    "    \"\"\"\n",
    "    Build MedSAM backbone safely on CPU if CUDA isn't available.\n",
    "    Some checkpoints were saved with CUDA tensors; we force map_location='cpu'.\n",
    "    \"\"\"\n",
    "    # Always build the SAM model with no weights first\n",
    "    sam = sam_model_registry[sam_type](checkpoint=None)\n",
    "\n",
    "    # Load checkpoint on CPU no matter what environment we're in\n",
    "    sd = torch.load(sam_ckpt, map_location=\"cpu\")\n",
    "    # Handle common wrappers\n",
    "    if isinstance(sd, dict) and \"state_dict\" in sd:\n",
    "        sd = sd[\"state_dict\"]\n",
    "    elif isinstance(sd, dict) and \"model\" in sd:\n",
    "        sd = sd[\"model\"]\n",
    "\n",
    "    # Strict where possible; relax if keys differ slightly across forks\n",
    "    missing, unexpected = sam.load_state_dict(sd, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(f\"[sam] missing keys: {missing[:8] if missing else []}\")\n",
    "        print(f\"[sam] unexpected keys: {unexpected[:8] if unexpected else []}\")\n",
    "\n",
    "    # Now wrap in your spatial-attn head\n",
    "    model = MedSAMSliceSpatialAttn(\n",
    "        sam_model=sam,\n",
    "        num_classes=num_classes,\n",
    "        proj_dim=proj_dim,\n",
    "        attn_dim=attn_dim,\n",
    "        head_hidden=head_hidden,\n",
    "        head_dropout=head_dropout,\n",
    "        use_pre_neck=use_pre_neck,\n",
    "    )\n",
    "\n",
    "    # Final device placement (CPU if no CUDA)\n",
    "    final_device = device if torch.cuda.is_available() and device.startswith(\"cuda\") else \"cpu\"\n",
    "    return model.to(final_device).eval()\n",
    "\n",
    "def load_ckpt_strict_filtered(model: torch.nn.Module, ckpt_path: str):\n",
    "    \"\"\"\n",
    "    Load a checkpoint but drop keys whose shapes don't match current model\n",
    "    (useful when proj_dim/head differ slightly).\n",
    "    \"\"\"\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    sd = sd.get(\"model\", sd)\n",
    "    own = model.state_dict()\n",
    "    filtered, dropped = {}, []\n",
    "    for k, v in sd.items():\n",
    "        if k in own and own[k].shape == v.shape:\n",
    "            filtered[k] = v\n",
    "        else:\n",
    "            dropped.append(k)\n",
    "    missing, unexpected = model.load_state_dict(filtered, strict=False)\n",
    "    print(f\"[load] loaded {ckpt_path}\")\n",
    "    if dropped:\n",
    "        print(f\"[load] dropped {len(dropped)} incompatible keys (e.g., {dropped[:6]})\")\n",
    "    if missing:\n",
    "        print(f\"[load] missing: {missing[:6]}\")\n",
    "    if unexpected:\n",
    "        print(f\"[load] unexpected: {unexpected[:6]}\")\n",
    "\n",
    "# ===================== Mask helpers =====================\n",
    "\n",
    "SUFFIX_RE = re.compile(r\"(_000\\d)?(\\.nii(\\.gz)?)$\")\n",
    "\n",
    "def path_root_from_channel_path(p: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip modality suffix like '_0000.nii.gz' from a channel path\n",
    "    to get a root ID (e.g., '10000_1000000').\n",
    "    \"\"\"\n",
    "    name = Path(p).name\n",
    "    name = SUFFIX_RE.sub(\"\", name)\n",
    "    return name\n",
    "\n",
    "def extract_slice_index(meta_sample: dict) -> int | None:\n",
    "    \"\"\"\n",
    "    Best-effort slice index extraction from the sample dict.\n",
    "    \"\"\"\n",
    "    for k in (\"slice_idx\", \"slice\", \"z\", \"z_idx\", \"index_z\"):\n",
    "        if k in meta_sample and meta_sample[k] is not None:\n",
    "            try:\n",
    "                return int(meta_sample[k])\n",
    "            except Exception:\n",
    "                pass\n",
    "    if \"meta\" in meta_sample and isinstance(meta_sample[\"meta\"], dict):\n",
    "        for k in (\"slice_idx\", \"slice\", \"z\", \"z_idx\", \"index_z\"):\n",
    "            if k in meta_sample[\"meta\"]:\n",
    "                try:\n",
    "                    return int(meta_sample[\"meta\"][k])\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return None\n",
    "\n",
    "def load_mask_slice(mask_dir: Path, root_name: str, slice_idx: int | None) -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    Load a 2D lesion mask slice (float32 in {0,1}) from <mask_dir>/<root>.nii(.gz).\n",
    "    Chooses the provided slice index; if None or out of bounds, uses the middle slice.\n",
    "    \"\"\"\n",
    "    f = mask_dir / f\"{root_name}.nii.gz\"\n",
    "    if not f.exists():\n",
    "        f = mask_dir / f\"{root_name}.nii\"\n",
    "        if not f.exists():\n",
    "            print(f\"[mask] missing for root={root_name}\")\n",
    "            return None\n",
    "    try:\n",
    "        vol = nib.load(str(f))\n",
    "        m = vol.get_fdata().astype(np.float32)\n",
    "        z = slice_idx if (slice_idx is not None and 0 <= slice_idx < m.shape[-1]) else m.shape[-1] // 2\n",
    "        sl = m[..., z]\n",
    "        sl = (sl > 0).astype(np.float32)\n",
    "        return sl\n",
    "    except Exception as e:\n",
    "        print(f\"[mask] failed to load {f}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e7e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Cell 2: Args (edit me) ===\n",
    "# Data\n",
    "MANIFEST   = \"/project/aip-medilab/shared/picai/manifests/slices_manifest.csv\"\n",
    "FOLDS_TEST = \"4\"  # e.g., \"4\" or \"3,4\"\n",
    "TARGET     = \"isup3\"  # [\"isup3\", \"binary_all\", \"binary_low_high\", \"raw\"]\n",
    "CHANNELS   = \"path_T2,path_ADC,path_HBV\"\n",
    "NUM        = 8           # used only if IDS is None\n",
    "IDS        = \"10012_1000012:6\"        # e.g., \"10034_1000345:18,10077_1000999:22\" or None\n",
    "SEED       = 42\n",
    "MASK_DIR   = \"/home/ewillis/projects/aip-medilab/shared/picai/picai_prepped_registered/labelsTr_lesion\"\n",
    "\n",
    "# Models\n",
    "SAM_TYPE   = \"vit_b\"   # [\"vit_b\", \"vit_l\", \"vit_h\"]\n",
    "SAM_CKPT   = \"/project/aip-medilab/ewillis/pca_contrastive/mri_model_medsam_finetune/work_dir/MedSAM/medsam_vit_b.pth\"\n",
    "BASELINE_CKPT = \"/home/ewillis/projects/aip-medilab/ewillis/pca_contrastive/mri_model_medsam_finetune_2D/results_no_loading/baseline/ckpt_best.pt\"\n",
    "ALIGNED_CKPT  = \"/home/ewillis/projects/aip-medilab/ewillis/pca_contrastive/mri_model_medsam_finetune_2D/results_no_loading/triplet/ckpt_head_best.pt\"\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "PROJ_DIM    = 512\n",
    "ATTN_DIM    = 256\n",
    "HEAD_HIDDEN = 256\n",
    "HEAD_DROPOUT= 0.1\n",
    "USE_PRE_NECK= True\n",
    "\n",
    "# Viz\n",
    "OVERLAY_ALPHA = 0.25   # faint overlay so white anatomy pops\n",
    "ROTATE_CCW    = True   # rotate for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e80451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewillis/projects/aip-medilab/ewillis/pca_contrastive/venv/lib/python3.11/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 805: MPS client failed to connect to the MPS control daemon or the MPS server (Triggered internally at /tmp/build_wheels_tmp.6999/python-3.11/torch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No matching case_ids found in dataset for provided --ids.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IDS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(IDS).strip()) > \u001b[32m0\u001b[39m:\n\u001b[32m     64\u001b[39m     ids_list = [t.strip() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(IDS).split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m t.strip()]\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     batch = \u001b[43msample_by_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     67\u001b[39m     batch = sample_batch(ds_test, n=NUM, seed=SEED)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36msample_by_ids\u001b[39m\u001b[34m(ds, ids_list)\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m picked:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo matching case_ids found in dataset for provided --ids.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m collate_resize_to_imgsize(picked)\n",
      "\u001b[31mValueError\u001b[39m: No matching case_ids found in dataset for provided --ids."
     ]
    }
   ],
   "source": [
    "# --- Sampling helpers missing in this notebook cell ---\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Resize each sample to IMG_SIZE so the batch stacks cleanly\n",
    "def collate_resize_to_imgsize(batch):\n",
    "    imgs, labels, metas = [], [], []\n",
    "    for s in batch:\n",
    "        x = s[\"image\"].unsqueeze(0)  # [1,C,H,W]\n",
    "        x = F.interpolate(x, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "        imgs.append(x)\n",
    "        labels.append(torch.as_tensor(s[\"label\"], dtype=torch.long))\n",
    "        metas.append(s)  # keep original dict for paths/ids\n",
    "    return {\"image\": torch.stack(imgs, 0), \"label\": torch.stack(labels, 0), \"meta\": metas}\n",
    "\n",
    "# Random sample N items from the dataset\n",
    "def sample_batch(ds, n, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = rng.choice(len(ds), size=min(n, len(ds)), replace=False)\n",
    "    batch = [ds[i] for i in idxs]\n",
    "    return collate_resize_to_imgsize(batch)\n",
    "\n",
    "# Optional: sample by explicit ids list if you’re using --ids\n",
    "# Expects each ds[i] to include 'case_id' in the returned dict.\n",
    "def sample_by_ids(ds, ids_list):\n",
    "    wanted = set(str(i) for i in ids_list)\n",
    "    picked = []\n",
    "    for i in range(len(ds)):\n",
    "        s = ds[i]\n",
    "        cid = str(s.get(\"case_id\", \"\"))\n",
    "        if cid in wanted:\n",
    "            picked.append(s)\n",
    "        if len(picked) == len(wanted):\n",
    "            break\n",
    "    if not picked:\n",
    "        raise ValueError(\"No matching case_ids found in dataset for provided --ids.\")\n",
    "    return collate_resize_to_imgsize(picked)\n",
    "\n",
    "# === Cell 3: Logic (display only; no files saved) ===\n",
    "set_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "chan_keys  = tuple(CHANNELS.split(\",\"))\n",
    "chan_names = (\"T2\", \"ADC\", \"HBV\")  # display names (must match order)\n",
    "\n",
    "# Dataset\n",
    "folds = [int(x.strip()) for x in FOLDS_TEST.split(\",\") if x.strip()]\n",
    "ds_test = PicaiSliceDataset(\n",
    "    manifest_csv=MANIFEST,\n",
    "    folds=folds,\n",
    "    use_skip=True,\n",
    "    label6_column=\"label6\",\n",
    "    target=TARGET,\n",
    "    channels=chan_keys,\n",
    "    missing_channel_mode=\"zeros\",\n",
    "    pct_lower=0.5, pct_upper=99.5,\n",
    "    cache_size=64,\n",
    ")\n",
    "\n",
    "# Choose samples\n",
    "if IDS and len(str(IDS).strip()) > 0:\n",
    "    ids_list = [t.strip() for t in str(IDS).split(\",\") if t.strip()]\n",
    "    batch = sample_by_ids(ds_test, ids_list)\n",
    "else:\n",
    "    batch = sample_batch(ds_test, n=NUM, seed=SEED)\n",
    "\n",
    "imgs, labels, metas = batch[\"image\"].to(device), batch[\"label\"].to(device), batch[\"meta\"]\n",
    "H, W = imgs.shape[-2], imgs.shape[-1]\n",
    "\n",
    "# Build + load models\n",
    "m_base = build_model(SAM_TYPE, SAM_CKPT, num_classes=NUM_CLASSES,\n",
    "                     proj_dim=PROJ_DIM, attn_dim=ATTN_DIM,\n",
    "                     head_hidden=HEAD_HIDDEN, head_dropout=HEAD_DROPOUT,\n",
    "                     use_pre_neck=USE_PRE_NECK, device=device)\n",
    "load_ckpt_strict_filtered(m_base, BASELINE_CKPT)\n",
    "\n",
    "m_align = build_model(SAM_TYPE, SAM_CKPT, num_classes=NUM_CLASSES,\n",
    "                      proj_dim=PROJ_DIM, attn_dim=ATTN_DIM,\n",
    "                      head_hidden=HEAD_HIDDEN, head_dropout=HEAD_DROPOUT,\n",
    "                      use_pre_neck=USE_PRE_NECK, device=device)\n",
    "load_ckpt_strict_filtered(m_align, ALIGNED_CKPT)\n",
    "\n",
    "# Forward once to get attention + logits (no grads)\n",
    "with torch.no_grad():\n",
    "    logits_b, emb_b, attn_b, _ = m_base(imgs, return_attn=True, return_feats=True, attn_upsample_to=(H, W))\n",
    "    logits_a, emb_a, attn_a, _ = m_align(imgs, return_attn=True, return_feats=True, attn_upsample_to=(H, W))\n",
    "\n",
    "pred_b = logits_b.argmax(1).cpu().numpy()\n",
    "pred_a = logits_a.argmax(1).cpu().numpy()\n",
    "y_true = labels.cpu().numpy()\n",
    "\n",
    "# Prepare masks\n",
    "mask_dir = Path(MASK_DIR)\n",
    "mask_list = []\n",
    "for meta in metas:\n",
    "    sample_path = None\n",
    "    for k in chan_keys:\n",
    "        if k in meta and meta[k] is not None:\n",
    "            sample_path = meta[k]; break\n",
    "        if \"paths\" in meta and isinstance(meta[\"paths\"], dict) and k in meta[\"paths\"]:\n",
    "            sample_path = meta[\"paths\"][k]; break\n",
    "    root = path_root_from_channel_path(str(sample_path)) if sample_path is not None else None\n",
    "    z = extract_slice_index(meta)\n",
    "    m2d = load_mask_slice(mask_dir, root, z) if root is not None else None\n",
    "    mask_list.append(m2d)\n",
    "\n",
    "# ---- Display per sample ----\n",
    "B = imgs.size(0)\n",
    "for i in range(B):\n",
    "    # Per-sample fig: rows = channels (T2/ADC/HBV), cols = Input | Mask | Base-Attn | Align-Attn\n",
    "    cols = 4\n",
    "    rows = 3\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3.2, rows*3.2))\n",
    "    if rows == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "    fig.suptitle(\"Sample {}  |  true={}  base_pred={}  aligned_pred={}\".format(\n",
    "        i, int(y_true[i]), int(pred_b[i]), int(pred_a[i])\n",
    "    ), fontsize=12)\n",
    "\n",
    "    # Common attention maps (0..1)\n",
    "    a_b = normalize01(attn_b[i].squeeze(0))\n",
    "    a_a = normalize01(attn_a[i].squeeze(0))\n",
    "\n",
    "    # Mask prep\n",
    "    mask = mask_list[i]\n",
    "    if mask is not None:\n",
    "        mask_rgb = (np.stack([mask, mask, mask], axis=-1) * 255).astype(np.uint8)\n",
    "    else:\n",
    "        mask_rgb = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "\n",
    "    # Channel loop\n",
    "    for r, (cidx, cname) in enumerate(zip([0,1,2], [\"T2\", \"ADC\", \"HBV\"])):\n",
    "        base_img = to_uint8_gray(imgs[i, cidx:cidx+1])\n",
    "\n",
    "        # Rotate for readability\n",
    "        if ROTATE_CCW:\n",
    "            base_img_r = rotate_ccw(base_img)\n",
    "            mask_rgb_r = rotate_ccw(mask_rgb) if mask is not None else mask_rgb\n",
    "            a_b_r = rotate_ccw(a_b); a_a_r = rotate_ccw(a_a)\n",
    "        else:\n",
    "            base_img_r = base_img\n",
    "            mask_rgb_r = mask_rgb\n",
    "            a_b_r = a_b; a_a_r = a_a\n",
    "\n",
    "        # Panels\n",
    "        axes[r,0].imshow(base_img_r); axes[r,0].set_title(f\"{cname} (input)\", fontsize=10)\n",
    "        axes[r,1].imshow(mask_rgb_r); axes[r,1].set_title(\"Lesion mask\", fontsize=10)\n",
    "        axes[r,2].imshow(overlay_red(base_img_r, a_b_r, alpha=OVERLAY_ALPHA)); axes[r,2].set_title(\"Baseline—Attn\", fontsize=10)\n",
    "        axes[r,3].imshow(overlay_red(base_img_r, a_a_r, alpha=OVERLAY_ALPHA)); axes[r,3].set_title(\"Aligned—Attn\", fontsize=10)\n",
    "\n",
    "        for cc in range(cols):\n",
    "            axes[r,cc].set_axis_off()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b4b274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ccea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"has_pool\": true,\n",
      "  \"has_attn_pool\": false,\n",
      "  \"pool.theta.weight\": [\n",
      "    256,\n",
      "    16,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"pool.gate.weight\": [\n",
      "    256,\n",
      "    16,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"proj.0.weight\": [\n",
      "    16\n",
      "  ],\n",
      "  \"proj.1.weight\": [\n",
      "    512,\n",
      "    16\n",
      "  ],\n",
      "  \"head.0.weight\": [\n",
      "    512\n",
      "  ],\n",
      "  \"head.1.weight\": [\n",
      "    256,\n",
      "    512\n",
      "  ],\n",
      "  \"all_pool_keys\": [\n",
      "    \"pool.gate.bias\",\n",
      "    \"pool.gate.weight\",\n",
      "    \"pool.norm.bias\",\n",
      "    \"pool.norm.weight\",\n",
      "    \"pool.score.bias\",\n",
      "    \"pool.score.weight\",\n",
      "    \"pool.theta.bias\",\n",
      "    \"pool.theta.weight\"\n",
      "  ],\n",
      "  \"all_proj_keys\": [\n",
      "    \"proj.0.bias\",\n",
      "    \"proj.0.weight\",\n",
      "    \"proj.1.bias\",\n",
      "    \"proj.1.weight\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch, re, sys, json\n",
    "\n",
    "ckpt_path = \"/home/ewillis/projects/aip-medilab/ewillis/pca_contrastive/mri_model_medsam_finetune_2D/results_no_loading/baseline/ckpt_best.pt\"\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "sd = sd.get(\"model\", sd)\n",
    "\n",
    "# strip potential 'module.' prefix from DDP\n",
    "if any(k.startswith(\"module.\") for k in sd):\n",
    "    sd = {k.replace(\"module.\",\"\",1): v for k,v in sd.items()}\n",
    "\n",
    "def shapes(prefix):\n",
    "    rgx = re.compile(rf\"^{re.escape(prefix)}\")\n",
    "    return {k: tuple(v.shape) for k,v in sd.items() if rgx.search(k)}\n",
    "\n",
    "out = {\n",
    "    \"has_pool\": any(k.startswith(\"pool.\") for k in sd),\n",
    "    \"has_attn_pool\": any(k.startswith(\"attn_pool.\") for k in sd),\n",
    "    \"pool.theta.weight\": tuple(sd.get(\"pool.theta.weight\", torch.empty(0)).shape) if \"pool.theta.weight\" in sd else None,\n",
    "    \"pool.gate.weight\":  tuple(sd.get(\"pool.gate.weight\",  torch.empty(0)).shape) if \"pool.gate.weight\" in sd else None,\n",
    "    \"proj.0.weight\":     tuple(sd.get(\"proj.0.weight\",     torch.empty(0)).shape) if \"proj.0.weight\" in sd else None,  # LayerNorm weight -> (C,)\n",
    "    \"proj.1.weight\":     tuple(sd.get(\"proj.1.weight\",     torch.empty(0)).shape) if \"proj.1.weight\" in sd else None,  # Linear -> (proj_dim, C)\n",
    "    \"head.0.weight\":     tuple(sd.get(\"head.0.weight\",     torch.empty(0)).shape) if \"head.0.weight\" in sd else None,\n",
    "    \"head.1.weight\":     tuple(sd.get(\"head.1.weight\",     torch.empty(0)).shape) if \"head.1.weight\" in sd else None,\n",
    "    \"all_pool_keys\": sorted([k for k in sd if k.startswith(\"pool.\")])[:12],\n",
    "    \"all_proj_keys\": sorted([k for k in sd if k.startswith(\"proj.\")])[:12],\n",
    "}\n",
    "print(json.dumps(out, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa2c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
